{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Cox model and Feature Importancy</h2>\n",
    "\n",
    "This notebook preprocesses data, including imputing and creating a cox model for the training dataset or implementing an existing model for the test dataset.\n",
    "\n",
    "### Choose name and data type in section 'User input'\n",
    "\n",
    "### Results are available in the section 'Results':\n",
    "- **Cox model C-index score**\n",
    "- **Feature importance plot**\n",
    "- **Kaplan-Meier plot**\n",
    "  - Here, you can choose what features should be presented on the plot.\n",
    "\n",
    "### Used Functions and Procedures from `/models/`:\n",
    "- Preprocess data steps:\n",
    "  1. Read dataframe and Create directory in `data_information/` to save data information\n",
    "  2. Extract features datatype and create datatype lists from `index_list_num_cat_ord_bin.csv`\n",
    "\n",
    "  3. Data correction:\n",
    "      1. `Nein` to `Nan` for ['dx_lightrat', 'l1_lightrat', 'l2_lightrat', 'dx_lightkap', 'l1_lightkap', 'l2_lightkap', 'dx_lightlam', 'l1_lightlam', 'l2_lightlam']\n",
    "      2. Convert numerical features type to float (original type: object)\n",
    "      3. Correction for 'kofscore':  20.21 to Nan\n",
    "      4. ECOG value 9.0 to Nan, bin category 0,1 - 0; 2,3,4 - 1\n",
    "      5. `dx_sex` to `dx_female_sex` 1/0,  bin category\n",
    "      6. Find outliers. Change for outliers 99-type input to Nan \n",
    "      7. BMI  \n",
    "          1. Find BMI and using possible BMI interval estimate the swap weight and height columns\n",
    "          2. BMI for dx, l1, l2 instead of height and weight features\n",
    "          3. Calculate relative BMI change: `l1_BMI_change`, `l2_BMI_change`\n",
    "\n",
    "      8. Data type columns:\n",
    "          1. Calculate duration of rtx\n",
    "          2. Calculate relative age when patient was diagnosed\n",
    "          3. Drop data `ddmmyy` type columns\n",
    "\n",
    "  4. Data mapping:\n",
    "      1. Replace 'Ja'/ 'Nein'/ 'Nicht getestet' to 1/ 0/ NaN (99) numerical value\n",
    "      2. Mapping object type data:\n",
    "          1. Sorting by number in the line:\n",
    "              Example: \n",
    "              '<3,5 mg/L': 1, '3,5 - 5,5 mg/L': 2, '>5,5 mg/L': 3\n",
    "          2. Sorting by Stadium I, II, III: giving indexes of mapping 1, 2, 3\n",
    "          3. For Unbekant Stadium: index 0\n",
    "\n",
    "  5. Dropping Features\n",
    "        \n",
    "      'training':\n",
    "\n",
    "          1. One unique value in column - 54 features was dropped\n",
    "          2. Imbalance feature drop and categories combination with small statistic (< 6%) -> Create list of combined categories per each feature\n",
    "              - Save data about categories combination to `/dictionaries/replaced_values.json`\n",
    "          3. Drop all repeated/ duplicate information/ high correlated values:\n",
    "              - drop_list = ['dx_weight', 'l1_weight', 'l2_weight', 'dx_height', 'l1_height','l2_height', 'l1_BMI','l2_BMI', 'l1_klinstudie','l2_klinstudie','dx_mikrogl','l1_mikrogl','l2_mikrogl','dx_albugem','l1_albugem','l2_albugem','l1_protg']\n",
    "              - Drop 'l1_protg' due to high correlation with 'l2_protg'\n",
    "          4. Drop outliers for ['dx_protg', 'l1_protg', 'l2_protg'] \n",
    "\n",
    "          after all steps, remain features list is saved in `/dictionaries/features_list_to_keep.json`\n",
    "        \n",
    "      'test':    \n",
    "\n",
    "          1. Open:\n",
    "          `/dictionaries/replaced_values.json`\n",
    "              - to combine categories inside each feature the same way as was done for training dataset \n",
    "          `/dictionaries/features_list_to_keep.json`\n",
    "              - to drop all features the same way as for training dataset\n",
    "              -if some column is missing, make a warning to retrain dataset\n",
    "\n",
    "          2. Remove outliers\n",
    "\n",
    "  6. Preprocess data\n",
    "      - 'test'\n",
    "        `preprocess_test_data(df_load, additional_name='')`\n",
    "          - to apply the same transformation that was learned from the training data\n",
    "        \n",
    "      - 'training'\n",
    "        \n",
    "        `preprocess_data(df_load, threshold_feature_drop=27, directory_name='models', additional_name='')`\n",
    "          - drop features with high missing values: threshold_feature_drop=27\n",
    "          - fixed imputers for different features categories:\n",
    "              cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "              num_imputer = KNNImputer(n_neighbors=10)\n",
    "              cat_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') \n",
    "              scaler = QuantileTransformer()\n",
    "          - split the data into training and testing sets\n",
    "          - return:\n",
    "              X_train = preprocessor.fit_transform(X_train) # model can learn the parameters of the transformations from this data\n",
    "              X_test= preprocessor.transform(X_test) #to apply the same transformation that was learned from the training data\n",
    "\n",
    "        `preprocess_data_choose_imputer(df_load, threshold_feature_drop=25, directory_name='models', additional_name='', cat_imputer=SimpleImputer(strategy='most_frequent'), num_imputer=KNNImputer(n_neighbors=10), scaler=QuantileTransformer())`\n",
    "          - function to choose imputers for each feature categories\n",
    "          - choose parameters values: \n",
    "              categorical and binomial (yes/no) feature imputer:\n",
    "                  cat_imputer=SimpleImputer(strategy='most_frequent'), \n",
    "              numerical feature imputer and numerical data scaler:\n",
    "                  num_imputer=KNNImputer(n_neighbors=10), \n",
    "                  scaler=QuantileTransformer()\n",
    "\n",
    "          - split the data into training (80%) and testing (20%) sets\n",
    "          - return:\n",
    "              X_train = preprocessor.fit_transform(X_train) # model can learn the parameters of the transformations from this data\n",
    "              X_test= preprocessor.transform(X_test) #to apply the same transformation that was learned from the training data\n",
    "\n",
    "- Cox Model, feature importance, and Kaplan-Meier plot steps:\n",
    "    1. `datatype_extraction_procedure(df, path_list=\"dictionaries/\", report = 0)`\n",
    "       procedure that returns back the lists of different types of variables from the dataset\n",
    "\n",
    "    2. `cox_model(X_train, X_test, data_type = 'training', penalizer=0.0001, l1_ratio=0.9)`\n",
    "       General Cox model\n",
    "\n",
    "    3. `cox_feature_importancy(X_train, X_test, clean_feature = True, data_type = 'training', feature_excluded_from_plot =[], size = (30,40), directory_name='', penalizer=0.0001, l1_ratio=0.9, additional_name='' )`\n",
    "       Cox model with feature importance plot.\n",
    "       This function allows creating the cox model with/without Lasso less significant features clean-up method and build the plot of important features \n",
    "\n",
    "    4. `Kaplan_Meier_plot(X_train, lim_min_percent=10, feature_groups_specific=[], plot_name='', directory_name='')`\n",
    "       Kaplan Meier curves for balanced categories\n",
    "\n",
    "    5. `color_style()`\n",
    "       color and style of plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all modules to the project to use functions\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# This should point to the project root where 'modules' directory is located\n",
    "project_root = Path().absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# importing modules\n",
    "from modules import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, PowerTransformer, Normalizer, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "Set_up_option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 User input  \n",
    "### Choose information about dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose 1/0 train/test data type\n",
    "data_type_train = 1  #1 - train,  0 - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct data_name in case of the different data type names\n",
    "\n",
    "#it choose data type: data_frame_type = training or test  \n",
    "#it choose data file name: data_name\n",
    "\n",
    "if data_type_train ==1:\n",
    "    #training data\n",
    "    data_name = 'training_df'  \n",
    "    data_frame_type = 'training' \n",
    "\n",
    "elif data_type_train ==0:\n",
    "    #test data\n",
    "    data_name = 'test_df'\n",
    "    data_frame_type = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************************** Automatical section *****************************************************************************\n",
    "# 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 read data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = read_df(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create directory to save information about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_directory_main(data_name, parent_directory = \"intermediate_report\")\n",
    "directory_name = create_directory_main(data_name, parent_directory = \"intermediate_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Correct data\n",
    "1. Convert \"Nein\" to `NaN` for the following columns:\n",
    "   - `['dx_lightrat', 'l1_lightrat', 'l2_lightrat', 'dx_lightkap', 'l1_lightkap', 'l2_lightkap', 'dx_lightlam', 'l1_lightlam', 'l2_lightlam']`\n",
    "2. Convert numerical features type to float (original type: object).\n",
    "3. Correction for `'kofscore'`: replace `20.21` with `NaN`.\n",
    "4. ECOG value `9.0` to `NaN`, bin category `0,1` - `0`; `2,3,4` - `1`.\n",
    "5. Convert `dx_sex` to `dx_female_sex` `1/0`, bin category.\n",
    "6. Find outliers. Change for outliers `99`-type input to `NaN`.\n",
    "7. BMI:\n",
    "    1. Find BMI and using possible BMI interval estimate the swap weight and height columns.\n",
    "    2. Implement BMI for `dx`, `l1`, `l2` instead of height and weight features.\n",
    "    3. Calculate relative BMI change: `l1_BMI_change`, `l2_BMI_change`.\n",
    "8. Data type columns:\n",
    "    1. Calculate duration of rtx.\n",
    "    2. Calculate relative age when the patient was diagnosed.\n",
    "    3. Drop data `ddmmyy` type columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_correction(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save information about amount of missing and unique values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_count(df,directory_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Data Mapping\n",
    "\n",
    "1. Replace categorical values with numerical values:\n",
    "   - Convert `'Ja'`/ `'Nein'`/ `'Nicht getestet'` to `1`/ `0`/ `NaN` (for `99`) respectively.\n",
    "   \n",
    "2. Mapping object type data:\n",
    "   1. Sorting by number in the line:\n",
    "      - Example: \n",
    "        - `'<3,5 mg/L'`: 1\n",
    "        - `'3,5 - 5,5 mg/L'`: 2\n",
    "        - `'>5,5 mg/L'`: 3\n",
    "   2. Sorting by Stadium:\n",
    "      - Assign indexes based on stadium number:\n",
    "        - `Stadium I`: 1\n",
    "        - `Stadium II`: 2\n",
    "        - `Stadium III`: 3\n",
    "   3. For `Unbekannt Stadium` (Unknown Stadium):\n",
    "      - Assign index 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_02_mapping, value_mapping = data_mapping(df, directory_name, data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_02_mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output mapping for values for each features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Drop features\n",
    "\n",
    "**'training':**\n",
    "\n",
    "1. **One unique value in column:**\n",
    "   - 54 features were dropped due to having only one unique value.\n",
    "   \n",
    "2. **Imbalance feature drop and categories combination with small statistics (< 6%): combine_categories=True**\n",
    "   - Create a list of combined categories per each feature.\n",
    "   - Save data about categories combination to `'helper/data_dictionary/replaced_values.json'`.\n",
    "   \n",
    "3. **Drop all repeated/duplicate information/high correlated values:**\n",
    "   - `drop_list = ['dx_weight', 'l1_weight', 'l2_weight', 'dx_height', 'l1_height','l2_height', 'l1_BMI','l2_BMI', 'l1_klinstudie','l2_klinstudie','dx_mikrogl','l1_mikrogl','l2_mikrogl','dx_albugem','l1_albugem','l2_albugem','l1_protg']`\n",
    "   - Drop `'l1_protg'` due to high correlation with `'l2_protg'`.\n",
    "   \n",
    "4. **Drop outliers for `['dx_protg', 'l1_protg', 'l2_protg']`.**\n",
    "\n",
    "   - After all steps, the remaining features list is saved in `'helper/data_dictionary/features_list_to_keep.json'`.\n",
    "\n",
    "**'test':**\n",
    "\n",
    "1. **Open:**\n",
    "   - `'helper/data_dictionary/replaced_values.json'` to combine categories inside each feature the same way as was done for the training dataset.\n",
    "   - `'helper/data_dictionary/features_list_to_keep.json'` to drop all features the same way as for the training dataset. If some column is missing, make a warning to retrain the dataset.\n",
    "\n",
    "2. **Remove outliers.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def drop_features(df,min_percent = 0.06, data_type ='train', combine_categories=True, data_name='', directory_name='', drop_list = ['dx_weight', 'l1_weight', 'l2_weight', 'dx_height', 'l1_height','l2_height',\t'l1_BMI'\t,'l2_BMI', 'l1_klinstudie','l2_klinstudie','dx_mikrogl','l1_mikrogl','l2_mikrogl','dx_albugem','l1_albugem','l2_albugem','l1_protg']):\n",
    "df_02_mapping_drop =  drop_features(df_02_mapping,min_percent = 0.06, data_type =data_frame_type, combine_categories=True, data_name=data_name, directory_name=directory_name, drop_list = ['dx_weight', 'l1_weight', 'l2_weight', 'dx_height', 'l1_height','l2_height',\t'l1_BMI'\t,'l2_BMI', 'l1_klinstudie','l2_klinstudie','dx_mikrogl','l1_mikrogl','l2_mikrogl','dx_albugem','l1_albugem','l2_albugem','l1_protg'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_02_mapping_drop.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Check unique sets of drugs. \n",
    "Could be used to combine drugs by sets in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_list = ['l2_', 'l1_']\n",
    "\n",
    "for l_element in l_list:\n",
    "    print(f'l_element: {l_element}')\n",
    "\n",
    "    columns_containing_drug = [column for column in df_02_mapping_drop.columns if 'drug' in column.lower()]\n",
    "    df_02_mapping_drugs = df_02_mapping_drop[columns_containing_drug]\n",
    "    l_columns = [col for col in df_02_mapping_drugs.columns if col.startswith(l_element)]\n",
    "\n",
    "    # Initialize a list to hold the results for each row\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in df_02_mapping.iterrows():\n",
    "        # Initialize a temporary list A for this row\n",
    "        A = []\n",
    "        \n",
    "        # Check each 'l_' column in this row\n",
    "        for col in l_columns:\n",
    "            if row[col] == 1:\n",
    "                A.append(col)\n",
    "        \n",
    "        # Add the list A to the results (one list per row)\n",
    "        results.append(A)\n",
    "\n",
    "    # Count the occurrences of each unique subset\n",
    "    unique_subset_counts = {}\n",
    "    for sublist in results:\n",
    "    # Sort and convert to tuple for dictionary key\n",
    "        sublist_tuple = tuple(sorted(sublist))\n",
    "        if sublist_tuple in unique_subset_counts:\n",
    "            unique_subset_counts[sublist_tuple] += 1\n",
    "        else:\n",
    "            unique_subset_counts[sublist_tuple] = 1\n",
    "\n",
    "    # Sort the subsets based on their counts\n",
    "    sorted_subsets = sorted(unique_subset_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the sorted subsets with their counts\n",
    "    for subset, count in sorted_subsets:\n",
    "        print(f\"Subset: {list(subset)}, Count: {count}\")\n",
    "\n",
    "    print(f'Number of unique subsets: {len(unique_subset_counts)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' #code in process to combine subsets\n",
    "#test\n",
    "l_element=[['l2_drug_1', 'l2_drug_3'], ['l2_drug_3', 'l2_drug_6'], ['l2_drug_3', 'l2_drug_6'],['l2_drug_1', 'l2_drug_3', 'l2_drug_3', 'l2_drug_6'], ['l2_drug_3', 'l2_drug_6']]\n",
    "set_list = [set(sublist) for sublist in l_element]\n",
    "combined_subsets = set()\n",
    "for i, subset_i in enumerate(set_list):\n",
    "    for j, subset_j in enumerate(set_list):\n",
    "        if i != j and subset_i.issubset(subset_j):\n",
    "            combined_subsets.add(tuple(l_element[i]))\n",
    "\n",
    "# Convert the set of tuples back to a list of lists\n",
    "combined_subsets = [list(subset) for subset in combined_subsets]\n",
    "\n",
    "combined_subsets'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preprocess , Feature importancy plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_frame_type=='training':\n",
    "    X_train, X_test = preprocess_data(df_02_mapping_drop, threshold_feature_drop=27)\n",
    "\n",
    "#def cox_feature_importancy(X_train, X_test, clean_feature = True ,data_type = 'training', feature_excluded_from_plot =[],size = (30,40), directory_name='',penalizer=0.0001, l1_ratio=0.9 ):\n",
    "\n",
    "    top_f_df, score_X_train, score_X_test, cph = cox_feature_importancy(X_train,X_test, clean_feature =True,data_type = data_frame_type, feature_excluded_from_plot =['bin__l2_rtpnotwnierenin','bin__l1_crabkritb','bin__dx_crabkritb','cat__l2_rtptheraentsch_3.0','cat__l2_prottyp_1.0','cat__l1_prottyp_2.0'],size = (30,40), directory_name=directory_name, penalizer=0.001, l1_ratio=0.9)\n",
    "    print(score_X_train, score_X_test)\n",
    "\n",
    "elif data_frame_type=='test':\n",
    "    df_test = preprocess_test_data(df_02_mapping_drop)\n",
    "    top_f_df, score_X_train, score_X_test, cph = cox_feature_importancy(_,df_test, clean_feature =True,data_type = data_frame_type,feature_excluded_from_plot =['bin__l2_rtpnotwnierenin','bin__l1_crabkritb','bin__dx_crabkritb','cat__l2_rtptheraentsch_3.0','cat__l2_prottyp_1.0','cat__l1_prottyp_2.0'],size = (50,50), directory_name=directory_name, penalizer=0.001, l1_ratio=0.9)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 C-index Evaluation\n",
    "\n",
    "The Concordance index (C-index) is a measure used to evaluate the predictive accuracy of survival models. It assesses the model's ability to correctly rank pairs of subjects based on their predicted survival times. The C-index is calculated for both the training and test datasets, with specific considerations for each:\n",
    "\n",
    "- **Training Dataset:**\n",
    "  - **Train Set:** Calculate the C-index on the training portion of the dataset to evaluate the model's fit and its ability to predict outcomes on data it has already seen.\n",
    "  - **Test Set:** Calculate the C-index on the test portion of the training dataset to assess the model's generalizability and its predictive accuracy on unseen data.  \n",
    "  \n",
    "\n",
    "- **Test Dataset:**\n",
    "  - **Test:** For the external or separate test dataset, calculate the C-index to further validate the model's predictive performance on new, unseen data. This step is crucial for understanding how the model performs in real-world scenarios or when applied to data from different sources or time periods.\n",
    "\n",
    "The C-index values obtained from these evaluations provide insights into the model's predictive accuracy and generalizability, guiding improvements and adjustments to enhance performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_X_train, score_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Results of another cox model without Lasso less significant features clean up method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to control the difference in score and effectivness of Lasso less significant features clean up method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_frame_type=='training':\n",
    "    score_X_train2,score_X_test2 = cox_model(X_train,X_test,data_type = data_frame_type, penalizer=0.001, l1_ratio=0.7)\n",
    "elif data_frame_type=='test':\n",
    "    score_X_train2,score_X_test2 = cox_model(_,df_test,data_type = data_frame_type, penalizer=0.001, l1_ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_X_train2, score_X_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Kaplan-Meier Plot\n",
    "\n",
    "The Kaplan-Meier plot is a non-parametric statistic used to estimate the survival function from lifetime data. To customize the Kaplan-Meier plot based on specific features, follow the steps below:\n",
    "\n",
    "1. **Choose Features to be Shown on Plot:**\n",
    "   - Identify and select the features you wish to visualize in the Kaplan-Meier plot. Group these features into a list named `feature_groups_specific`. This allows for a focused analysis on the impact of these specific features on survival.\n",
    "   \n",
    "2. **Plot Name:**\n",
    "   - Assign a meaningful name to your plot for easy identification. Store this name in a variable called `plot_name`. This name will be used as the title of the Kaplan-Meier plot or for reference purposes in your analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_frame_type=='test':\n",
    "    X_train=df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kaplan_Meier_plot(X_train,lim_min_percent=10,feature_groups_specific=[], plot_name='', directory_name=''):\n",
    "Kaplan_Meier_plot(X_train,lim_min_percent=10,feature_groups_specific = ['bin__l2_drug_5','bin__l1_drug_6','bin__l2_drug_4'],plot_name='Drugs', directory_name=directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Kaplan_Meier_plot(X_train,lim_min_percent=10,feature_groups_specific = ['cat__dx_einrid_1.0','cat__dx_einrid_3.0'],plot_name='Clinic', directory_name=directory_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "mlg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
